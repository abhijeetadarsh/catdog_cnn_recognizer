# -*- coding: utf-8 -*-
"""catdog_cnn_recognizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v1Kkxv6z4Z5n78Q_sNG-R6Pg6gkTQVZP

# Cats vs Dogs Image Classification using Convolutional Neural Networks (CNN)

This notebook demonstrates how to build and train a CNN model to classify images of cats and dogs.

## Setup and Data Preparation
"""

# Set up the Kaggle dataset
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!mkdir -p data
!kaggle datasets download -d tongpython/cat-and-dog -p data/
!unzip data/cat-and-dog.zip -d data/
!rm data/cat-and-dog.zip

import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout
import matplotlib.pyplot as plt

def process(image, label):
    """Normalize the image data"""
    image = tf.cast(image/255, tf.float32)
    return image, label

# Load and preprocess the training data
train_data = keras.utils.image_dataset_from_directory(
    directory = '/content/data/training_set/training_set',
    labels = 'inferred',
    label_mode = 'int',
    batch_size = 32,
    image_size = (256, 256)
)
train_data = train_data.map(process)

# Load and preprocess the validation data
validation_data = keras.utils.image_dataset_from_directory(
    directory = '/content/data/test_set/test_set',
    labels = 'inferred',
    label_mode = 'int',
    batch_size = 32,
    image_size = (256, 256)
)
validation_data = validation_data.map(process)

"""## Initial CNN Model"""

def create_model():
    model = Sequential([
        Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3)),
        MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),
        Conv2D(64, kernel_size=(3, 3), padding='valid', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),
        Conv2D(128, kernel_size=(3, 3), padding='valid', activation='relu'),
        MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    return model

# Create and compile the initial model
model = create_model()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the initial model
history = model.fit(train_data, epochs=10, validation_data=validation_data)

"""## Visualize Training Results"""

def plot_history(history, metric):
    plt.plot(history.history[metric], color='red', label='train')
    plt.plot(history.history[f'val_{metric}'], color='blue', label='validation')
    plt.legend()
    plt.title(f'Model {metric}')
    plt.show()

plot_history(history, 'accuracy')
plot_history(history, 'loss')

"""## Improved Model with Regularization"""

def create_improved_model():
    model = Sequential([
        Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3)),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),
        Conv2D(64, kernel_size=(3, 3), padding='valid', activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),
        Conv2D(128, kernel_size=(3, 3), padding='valid', activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid'),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.1),
        Dense(64, activation='relu'),
        Dropout(0.1),
        Dense(1, activation='sigmoid')
    ])
    return model

# Create and compile the improved model
model_2 = create_improved_model()
model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the improved model
history_2 = model_2.fit(train_data, epochs=10, validation_data=validation_data)

"""## Visualize Improved Model Results"""

plot_history(history_2, 'accuracy')
plot_history(history_2, 'loss')

"""## Notes on Reducing Overfitting

1. Add more data
2. Data Augmentation
3. L1/L2 Regularization
4. Dropout (implemented)
5. Batch Normalization (implemented)
6. Reduce model complexity
"""